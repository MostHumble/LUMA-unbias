{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a885a9d8-8a53-45fe-abe0-01decda4b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769bbfca-2589-45a8-bd14-67ddb9699eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c84612cd-b463-4625-829d-005e3a3ef397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (86 > 64). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3000.000000\n",
      "mean       84.026667\n",
      "std        10.794047\n",
      "min        52.000000\n",
      "25%        77.000000\n",
      "50%        83.000000\n",
      "75%        91.000000\n",
      "max       144.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load captions\n",
    "captions_path = 'captions/captions_cifar100_man_woman_baby_girl_boy.pkl'\n",
    "cifar_captions = pd.read_pickle(captions_path)\n",
    "\n",
    "# Load the processor and tokenizer\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# Calculate token counts for each caption\n",
    "token_counts = [len(tokenizer(caption, truncation=False, max_length=None)['input_ids']) for caption in cifar_captions]\n",
    "\n",
    "# Summary statistics\n",
    "token_stats = pd.Series(token_counts).describe()\n",
    "\n",
    "# Output the stats\n",
    "print(token_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8e055ce-7392-44f1-9b6e-0381d499daa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipTokenizer(name_or_path='google/siglip-so400m-patch14-384', vocab_size=32000, model_max_length=64, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t1: AddedToken(\"</s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<unk>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uses Sentencepiece: https://github.com/huggingface/transformers/blob/main/src/transformers/models/siglip/tokenization_siglip.py\n",
    "tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luma_unbias",
   "language": "python",
   "name": "luma_unbias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
